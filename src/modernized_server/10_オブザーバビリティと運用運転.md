# 10_オブザーバビリティと運用運転 ✓

- RUN_ID: `20251122T071146Z`
- 期間: 2025-12-09 09:00 〜 2025-12-13 09:00 (JST)
- 優先度: Medium / 緊急度: Low
- エージェント: gemini cli
- YAML ID: `src/modernized_server/10_オブザーバビリティと運用運転.md`

## 参照チェーン（遵守）
1. `AGENTS.md`
2. `docs/web-client/README.md`
3. `docs/server-modernization/phase2/INDEX.md`
4. `docs/managerdocs/PHASE2_MANAGER_ASSIGNMENT_OVERVIEW.md`
5. 関連チェックリスト: `docs/managerdocs/PHASE2_SERVER_FOUNDATION_MANAGER_CHECKLIST.md`

## 禁止事項 / 前提
- `server/` 配下および Legacy 資産（`client/` `common/` `ext_lib/`）の変更は禁止。参照のみ。
- Python スクリプトの実行は禁止（明示指示がある場合のみ例外）。
- WebORCA 接続が必要な検証は行わない。監査ログ・メトリクス・トレースはモダナイズ版サーバーの構成と Runbook 整備に限定する。
- RUN_ID は本タスクの `20251122T071146Z` に統一し、ログ・証跡・DOC_STATUS へ同一値で記録する。

## ゴール
- Micrometer/Prometheus/Loki/OTLP など観測データの出力先と保持期間を定義し、主要メトリクス・監査・トレースのアラート閾値を提示する。
- 障害対応 runbook と運用ハンドオフチェックリストを整備し、オンコール／定期点検で即参照できる形にする。
- 監査ログと合わせたデータ保持ポリシーを RUN_ID 付きで `docs/server-modernization/phase2/operations/logs/` に残し、Evidence/アーティファクト導線を揃える。

## スコープ / 非スコープ
- スコープ: モダナイズ版サーバーの観測データ（メトリクス/ログ/トレース）設計、保持期間とアラート閾値の定義、障害対応 runbook・ハンドオフチェックリスト作成、DOC_STATUS/ハブ更新。
- 非スコープ: Web クライアント UI 変更、Legacy サーバー設定変更、本番 ORCA 接続やデータ投入、CI/CD 実行。

## 期待アウトプット（DoD）
1. メトリクス/ログ/トレースの出力先・保持期間・アラート閾値をまとめた運用ノートが `docs/server-modernization/phase2/operations/logs/20251122T071146Z-tracing.md`（旧: `.../20251121T131725Z-observability.md`）に作成されている。
2. 障害対応 runbook（検知→一次切り分け→復旧→エスカレーション）と運用ハンドオフチェックリストが同ログに掲載され、アラート名と連動している。
3. 監査ログ（`d_audit_event`）と観測データのデータ保持ポリシーが明記され、Evidence/アーティファクト保存パス（例: `artifacts/observability/20251122T071146Z/`）が定義されている。
4. RUN_ID=`20251122T071146Z` を `DOC_STATUS.md` 備考欄と関連ハブへ反映できるよう準備されている。
5. Stage/Prod 実値待ちテンプレ（Vault パス・UID・recording rule ラベルセットの取得手順）は `artifacts/observability/20251122T071146Z/README.md` に整備済み。実値入手後は本章と同 RUN_ID で置換する。

## タスク分解
- **A. 観測対象と保持ポリシー定義**
  - Micrometer（REST/JDBC/SSE）、WildFly/Undertow、監査ログ、OTLP トレースの出力先と保持期間を決定し、Ops/オンコール向けの表を作成する。
  - Prometheus/Loki/Elastic/Grafana/Alertmanager/PagerDuty の関連付け（job 名、ダッシュボード UID、ルーティング先）を整理する。
- **B. アラート設計と runbook**
  - エラーレート・レイテンシ・接続枯渇・監査フォーマット崩れ・OTLP 不能の代表シグナルを選定し、Warning/Critical の閾値と通知先を定義する。
  - 検知→切り分け→復旧→エスカレーションの手順、証跡取得コマンド、PoC/Stage/Prod の差分を runbook とハンドオフチェックリストへ記載する。
- **C. ドキュメント・棚卸し**
  - `docs/server-modernization/phase2/operations/logs/20251122T071146Z-tracing.md`（旧: 20251121T131725Z-observability）を作成し、A/B の成果と Evidence へのパスをまとめる。
  - `docs/web-client/planning/phase2/DOC_STATUS.md` に RUN_ID/ログパス/備考を追記し、必要に応じてハブ（README/INDEX）へリンクを追加する。

## 証跡配置
- ログ: `docs/server-modernization/phase2/operations/logs/20251122T071146Z-tracing.md`（旧 RUN の `...20251121T131725Z-observability.md` も参照可）
- アーティファクト: `artifacts/observability/20251122T071146Z/{dashboards,alerts,evidence}/` を起点として Evidence を集約する。ダミー UID/ルーティングキーの整理は `artifacts/observability/20251122T071146Z/README.md` を参照。

## スケジュール目安
- 12/09 (Day1): 観測対象・保持ポリシーの草案作成、アラート案の洗い出し。
- 12/10 (Day2): アラート閾値と通知経路、runbook/ハンドオフチェックリストの初版作成。
- 12/11 (Day3): Evidence/アーティファクト構造整理、ログドキュメント仕上げ。
- 12/12 09:00 まで: DOC_STATUS 反映と引き継ぎメモ記載、残課題整理。

## リスクと留意点
- Prometheus/Loki/Grafana の環境差異により保持期間が揃わないリスク → 各環境のデフォルト値と上書きパラメータを並記し、最小共通期間をアラート/証跡の前提にする。
- 監査ログとメトリクスの突合が未自動化 → traceId/RequestId を必須フィールドとして runbook に照合手順を明記し、ハンドオフチェックリストへ確認項目を追加する。
- OTLP 収集経路が未構成の場合の運用ブレ → OTLP 未接続時のフォールバック（Prometheus/Loki 単独運用）と検知方法を runbook に記載する。

## SLO/SLA 指標と収集ポイント（RUN_ID=`20251122T071146Z`）
- **REST API レイテンシ**: `http_server_requests_seconds` の `quantile=p95/p99` を環境×エンドポイント×メソッドで集計。SLO: p95 < 300ms、p99 < 800ms（5分スライディング）。収集: Micrometer→Prometheus `job=modernized-api`。
- **REST API 成功率**: `rate(http_server_requests_seconds_count{status!~"5.."}[5m]) / rate(http_server_requests_seconds_count[5m])`。SLO: 99.5%/5m、SLA: 99.0%/日。収集: Prometheus + Alertmanager、メソッド/パスは `uriTemplate` ラベルでロールアップ。
- **バックグラウンドジョブ遅延**: `job_queue_wait_seconds`（enqueue→start）と `job_run_duration_seconds`（start→end）の p95/p99。SLO: enqueue待ち p95 < 30s、実行 p95 < 5s。収集: Micrometer Timer（`job=async-worker`）＋ Grafana ダッシュボード UID `obs-job-latency` へ紐付け。
- **エラー率（例外・5xx）**: `http_server_requests_seconds_count{status=~"5.."}` と `application_exceptions_total` を 5m 窓で計算。SLO: 5xx < 0.5%/5m、Critical: 1% 超過 10 分継続で PagerDuty。
- **ORCA 連携失敗率**: `orca_bridge_calls_total{result!="00"}` / 全呼び出し。SLO: <1.0%/15m、SLA: <0.5%/日。補足: `result` は Api_Result / HTTP ステータスの正規化コード、`facility` `api` ラベルで分解。
- **SSE/ストリーミング中断率**: `sse_disconnect_total{reason!="client"}` / `sse_connections_total`。SLO: <0.2%/日。収集: Undertow カスタムメトリクス→Prometheus。
- **データベース接続余裕度**: `db_pool_active_connections / db_pool_max_connections` p95 < 0.7、Critical: p99 > 0.85。収集: HikariCP Micrometer バインダ。

### 主要指標サマリと収集ポイント（追補 20251122T071146Z）
- **p95/p99 レイテンシ**: `/api/*` を path template でロールアップし、環境別 `deployment.environment` と `facility` ラベルで分解。PromQL 例 `histogram_quantile(0.95, sum by(le,uriTemplate)(rate(http_server_requests_seconds_bucket[5m])))` を Alertmanager A1 と同一定義に合わせる。
- **ジョブ遅延/滞留**: `job_queue_wait_seconds` と `queue_depth` をセットで収集し、`job_type` ラベルで前処理/後処理/外部連携を区別。5m 移動平均で A4 を評価し、Grafana では p95/p99 をカード表示。
- **エラー率/例外**: 5xx と `application_exceptions_total` を `version` ラベルで分岐させ、デプロイ後 15 分の抑止条件（`deployed_within=15m`）を共通化。
- **ORCA 失敗率**: `orca_bridge_calls_total` を `result` 正規化コード別にカウントし、`result="timeout"` と `result="http_5xx"` を別アラートの抑止条件として保持。
- **SSE 中断率**: `sse_disconnect_total{reason!="client"}` を分子、`sse_connections_total` を分母として recording rule を作成し、`facility` と `endpoint` の 2 軸でダッシュボードに表示。A9 と同定義。
- **DB プール枯渇余裕度**: `db_pool_active_connections / db_pool_max_connections` の p95/p99 を `pool` ラベル別に保存し、夜間ジョブ帯のベースラインを記録。A3 のしきい値と同じ式を記載。

## ログ・トレース方針（構造化・相関・PII）
- **構造化ログ**: JSON Lines 形式、必須キー `timestamp` `level` `service` `env` `traceId` `spanId` `requestId` `facilityId` `actorType` `endpoint` `status` `latencyMs`。アプリログは Loki、監査/セキュリティは Elastic（PII マスク済）へ送信。
- **相関 ID**: 受信時に `requestId` を生成し `traceparent` と併用。ORCA 呼び出しには `X-Request-Id` を透過し、レスポンスログと `d_audit_event` に同 ID を記録してメトリクスと突合。
- **PII マスキング**: 氏名・住所・電話・保険者番号・診療内容はログに出さない方針。やむを得ず出力する場合は `hash(PII + salt per env)` を使用し可逆化禁止。Audit ログも `payload_truncated=true` として最大 256 文字までに制限。
- **サンプリングと保持**: アプリログ 14 日、監査ログ 90 日、トレース（OTLP→Tempo/Jaeger）は Stage=14 日 / Prod=30 日 を基準とし、メトリクス 30 日を原則。環境差異がある場合は最短値（14 日）を基準にアラート評価期間を設定。
- **収集パイプライン**: Fluent Bit で構造化ログを Loki/Elastic に二重送信。OTLP Collector は `service.name` `deployment.environment` ラベルを必須とし、`traceId` が欠損したログは `log_processing_errors_total` で検知し Warning 通知。
- **アラート連携**: Alertmanager で Critical を PagerDuty（ops-primary）、Warning を Slack `#modernized-ops` へルーティング。ルール名には RUN_ID を含め Evidence 追跡を容易化。

### ログ設計追補（構造化/相関/PII 強化 20251122T071146Z）
- **フィールド標準化**: 監査ログ `d_audit_event` とアプリログの共通フィールドとして `requestId` `traceId` `facilityId` `userRole` `endpoint` `status` を必須化し、Loki/Elastic 双方向で同スキーマを維持する。
- **相関 ID 伝播**: Web クライアントからの `traceparent` + `X-Request-Id` をそのまま ORCA 呼び出しへ透過し、JMS でもヘッダへ格納。欠損時は Collector で `correlation_missing_total` をインクリメントし Warning を発火。
- **PII マスク実装**: ログ出力前に氏名・住所・保険者番号・診療内容などのフィールドを `sha256(value + env_salt)` でハッシュ化し、原文は出力しない。監査ログは 256 文字を上限とし、`payload_truncated=true` を必ず付与する。
- **構造化 JSON 方針**: 1 行 1 JSON を厳守し、`latencyMs` `result` `api` `version` `env` をトップレベルに配置。Loki では `service`/`env`/`facilityId` を label 化し、Elastic では PII マスク済みフィールドのみを index 化する。
- **保存期間とローテーション**: Loki=14 日、Elastic（監査）=90 日、Tempo=Stage14 日/Prod30 日（compactor 24h）、Jaeger=Stage/Prod 30 日 rollover 24h、Prometheus=30 日。環境差異がある場合は最短値に合わせアラート評価窓を設定し、ローテーション設定は `docs/server-modernization/phase2/operations/logs/20251122T071146Z-metrics.md` へ実測メモを残す。
  - 環境別の Tempo/Jaeger 保持・コンパクション・OTLP Collector 設定は `docs/server-modernization/phase2/operations/logs/20251122T071146Z-tracing.md#tempojaeger-環境別設定棚卸し2025-11-22-実値反映` を参照する（Stage/Prod 実値反映済み）。

## 関連手順書（RUN_ID=`20251122T071146Z`）
- **Web クライアント運用**
  - [`docs/web-client/operations/otel-webclient-init.md`](../../docs/web-client/operations/otel-webclient-init.md): ブラウザ OTel 初期化と TraceContext 伝搬・リトライ時 TraceId 継承・エラー画面表示のテンプレート。
  - [`docs/web-client/operations/LOCAL_BACKEND_DOCKER.md`](../../docs/web-client/operations/LOCAL_BACKEND_DOCKER.md): モダナイズ版サーバーを含むローカル backend 起動手順（Prometheus/Loki/Tempo 連携時の確認ポイントを含む）。
  - [`docs/web-client/operations/TEST_SERVER_DEPLOY.md`](../../docs/web-client/operations/TEST_SERVER_DEPLOY.md): テスト環境へのデプロイ手順とロールバックフロー。メトリクス監視/ログ収集の有効化確認に利用。
  - [`docs/web-client/operations/DEV_MSW_MOCKS.md`](../../docs/web-client/operations/DEV_MSW_MOCKS.md): MSW モックの起動/切替手順。観測データの差分確認時に利用。
  - [`docs/web-client/operations/mac-dev-login.local.md`](../../docs/web-client/operations/mac-dev-login.local.md): 開発用 ORCA 接続設定。WebORCA トライアル以外の本番経路や `curl --cert-type P12` は使用禁止。
- **Phase2 モダナイズ運用・観測**
  - [`docs/server-modernization/phase2/operations/WILDFLY33_MICROMETER_OPERATIONS_GAP.md`](../../docs/server-modernization/phase2/operations/WILDFLY33_MICROMETER_OPERATIONS_GAP.md): WildFly 33 + Micrometer の計測ギャップと埋め方。
  - [`docs/server-modernization/phase2/operations/TRACE_PROPAGATION_CHECK.md`](../../docs/server-modernization/phase2/operations/TRACE_PROPAGATION_CHECK.md): `traceparent`/`requestId` の伝播確認手順と検証コマンド。
  - [`docs/server-modernization/phase2/operations/MODERNIZED_API_DOCUMENTATION_GUIDE.md`](../../docs/server-modernization/phase2/operations/MODERNIZED_API_DOCUMENTATION_GUIDE.md): REST/ORCA Runbook のハブ。API 契約・監査ログ方針のリンク起点。
  - [`docs/server-modernization/phase2/operations/ORCA_CONNECTIVITY_VALIDATION.md`](../../docs/server-modernization/phase2/operations/ORCA_CONNECTIVITY_VALIDATION.md): ORCA 接続検証/証跡採取手順（開発用 ORCA のみ）。観測ログと突合する際の前提として参照。
  - [`docs/server-modernization/phase2/operations/SECURITY_SECRET_HANDLING.md`](../../docs/server-modernization/phase2/operations/SECURITY_SECRET_HANDLING.md): シークレット管理と監査ログ取り扱い方針。メトリクス/ログとの突合時に参照。
- UI 実装確認済み: Web クライアントの TraceId 表示とリトライ継承を RUN_ID=`20251122T071146Z` で確認。実バックエンド 500 応答時の TraceErrorBoundary／TraceNoticeCenter 同一 ID スクショを `artifacts/observability/20251122T071146Z/traceid-error-boundary.png` に保存。
- **留意**: Legacy 資産（`client/` `common/` `ext_lib/`）と関連手順書は参照専用とし、変更や運用作業は実施しない。

## 混在環境インシデント運用（RUN_ID=`20251122T071146Z`）
- 前提: Web クライアントが Legacy/モダナイズ版サーバーを並走させる期間の運用。切り戻し・段階的ロールアウトを想定し、RACI を統一する。
- フロー: (1) 検知（アラート/利用者報告）→ (2) トリアージ（影響面と経路判定）→ (3) 暫定対応（切り戻し・Feature Flag・Rate Limit）→ (4) 恒久対策（修正・再発防止・ポストモーテム）。

### 検知チャネルと判定軸（混在環境）
- アラート起点: A1/A2/A3/A8/A9/A10（モダナイズ）＋ Legacy 監視（WildFly10/Syslog）を PagerDuty/Slack に集約し、通知本文に `environment` と `facility` を必須とする。
- ユーザー報告/CS 起点: UI 画面名と TraceId を必須入力にし、Legacy/Modernized どちらで再現するかを一次切り分けのトリアージ票に記入する。
- システム指標: `version` ラベル差分（canary / legacy-backend）と `routing_backend`（legacy|modernized）を Grafana で並列表示し、影響面の仮説を 5 分以内に立てる。

### 暫定対応オプション（優先順）
1. Feature Flag で UI 機能を段階的に停止/制限（該当 API を Modernized→Legacy へフォールバック）。
2. リバースプロキシの経路切替: `/api/*` 単位で Legacy へ切り戻し、同時に `routing_backend=legacy` ラベルでメトリクスを記録して SLO 消費を測定。
3. Rate Limit/サーキットブレーカ: ORCA 呼び出し失敗率が急増した場合に Modernized 側のみ一時抑止し、Legacy は維持したまま CS へ影響を共有。
4. 監査/Evidence 取得: `docs/server-modernization/phase2/operations/logs/20251122T071146Z-incident.md` に RUN_ID 付きで初動ログ（発生時刻・影響経路・暫定対応）を記録し、ポストモーテムへ引き継ぐ。

### RACI（検知→トリアージ→暫定対応→恒久対策）
| フェーズ/主要アクション | Webクライアント担当 | モダナイズ版サーバー担当 | オンコール（Ops/SRE） | プロダクト責任者 | セキュリティ/監査 |
| --- | --- | --- | --- | --- | --- |
| 1. 検知: PagerDuty/Slack アラート確認、ユーザー報告一次受付 | C | C | R/A | I | I |
| 2. トリアージ: 影響範囲推定（Facility/Endpoint/UI）、経路判定（Legacy/Modernized）、SLO 逸脱確認 | R | R | C | I | I |
| 3. 暫定対応: a) Feature Flag で UI 機能停止/限定、b) リバースプロキシで Legacy へ切り戻し、c) ルーティング変更手順の実行と通知 | R | C | A | I | I |
| 4. 恒久対策: コード修正・テスト・リリース計画、移行計画の更新 | C | R/A | I | I | I |
| 5. 証跡・連絡: Evidence 集約、ハブ更新、顧客/内部報告テンプレ反映 | R | R | C | A | I |

### ポストモーテム（必須項目）
- 事象概要（開始/検知/収束の UTC/JST タイムスタンプ、影響ユーザー数/施設数、影響経路=Legacy/Modernized/双方）。
- 事象分類と SLO/SLA への影響（該当指標、誤検知/検知遅延の有無、例外申請の要否）。
- 根本原因（5 Whys）と寄与要因（構成差分、Feature Flag 設計、リリース手順、監視カバレッジ）。
- 検知・エスカレーションのギャップ（どのシグナルが欠落/閾値不適切だったか、対応する新ルール案）。
- 恒久対策とオーナー/期限（DRI=責任者、Due=日付）。暫定対応の解除条件を明記。
- 証跡パス（例: `docs/server-modernization/phase2/operations/logs/20251122T071146Z-incident.md`、関連メトリクス/ログ/トレース/ダッシュボード）。

### SLO 例外申請手順（混在環境向け）
1. 例外リクエストを `#modernized-ops` へテンプレート投稿（RUN_ID、対象 SLO 名、期間、想定影響、暫定対応内容、リスク評価を必須記載）。
2. オンコール（Ops/SRE）が受付し、プロダクト責任者が承認（A）。セキュリティ/監査は I として記録のみ。
3. 承認後は Alertmanager の該当ルールに `slo_exception=true` ラベルを一時付与し、終了日時を明示。Grafana Annotation に RUN_ID と理由を追記。
4. 例外期間終了後 24 時間以内にポストモーテムへ結果を反映し、ラベル/Annotation を除去する。恒久対策の期限が過ぎた場合は自動で例外無効とし、再申請を必須とする。

## Tracing (OpenTelemetry) 強化（RUN_ID=`20251122T071146Z`）
- 目的: Web クライアント→モダナイズ API→ORCA/JMS キューの境界を一気通貫で可視化し、遅延/失敗を最短経路で特定する。

### スパン設計（サービス・API 境界・キュー）
- **エントリポイント**: API Gateway / WildFly ingress で `http.server` スパンを開始し、`service.name=modernized-api` `deployment.environment` `client.address` を付与。REST リソース層ではルート単位の子スパン（`HTTP <method> <route>`）を作り、`baggage.facility_id` と `baggage.user_role` に最小限のコンテキストを格納（PII 禁止）。
- **ドメインサービス/外部呼び出し**: ORCA REST などの外部 HTTP を `http.client` スパン、JPA/JDBC を `db.client` スパン、SSE/ストリーミングを `messaging.publish` スパンで分離。リトライは同一 TraceId で span.link を使用し、`retry.count` 属性を付ける。
- **キュー/バッチ**: JMS 送信時に `traceparent` / `tracestate` をメッセージヘッダへ書き込み、コンシューマーは `messaging.consume` を親スパンとして開始。キュー投入→ジョブ完了までを 1 Trace で束ね、HTTP 応答とは span.link で関連付けて UI のジョブ進捗と突合できるようにする。

#### 追加ポリシー（RUN_ID=`20251122T071146Z`）
- **命名規約/属性最小セット**: span 名は `<layer>::<operation>`（例: `resource::GET /patients/{id}`, `domain::issue_prescription`, `messaging::consume job-type=export`）。共通属性として `facility.id` `request.id` `user.role` `deployment.environment` を必須化し、ORCA 呼び出し時は `orca.api`（例:`acceptmodv2`）と `orca.result` を付与。
- **ステータスとエラー記録**: `SpanStatus` はアプリケーションの結果に合わせる（5xx=ERROR, 4xx=OK+error.type/description）。例外スタックは `event` に限定し、PII を含むメッセージは `hash` 化して格納。
- **ゲートウェイ→サービス境界**: API Gateway で開始した `http.server` を WildFly で `Link` 受け渡しし、逆プロキシでの再試行は `span.kind=SERVER` のまま `retry.attempt` を加算。GraphQL/Batch ファサードはサブスパンで分解し、UI ロード時にボトルネックを可視化。
- **JMS/キュー詳細**: `messaging.system=jms`、`messaging.operation=send|process` を明示し、メッセージ ID と `traceparent` をヘッダに保存。デッドレター行きは `messaging.delivery_state=dead_letter`、リトライ判定は `messaging.redelivered=true` で表現し、Alert A4/A5 の調査を容易にする。
- **ジョブ境界の相関**: 非同期ジョブは「HTTP 受付 → enqueue → worker start → worker finish」を 1 Trace で鎖状に保持し、UI には enqueue 時の TraceId を返却。後段ジョブが別キューを呼ぶ場合は `span.link` で親ジョブを接続し、Tempo/Jaeger の `Related Traces` で辿れるようにする。

### サンプリング方針
- **環境別**: Dev/Stage は `parentbased_always_on`。Prod は `parentbased_probability` 5% を基本とし、`http.status_code>=500`、`messaging.message.failed=true`、`audit.level=ERROR` は tail-sampling で 100% 残す。
- **リクエスト種別**: 認証・会計・監査系エンドポイント（/login, /billing, /audit）は 100% 取得期間を 7 日ロール。バッチ/エクスポートはジョブ単位で全量採取し、完了後 30 分以内にスパンを閉じる。
- **レート制御**: 施設ごとに毎秒 50 リクエスト分までサンプル許容量を設定し、超過時は `sampling.decision=DROP` をメトリクス化してアラート（Warning 5 分平均 10% 超、Critical 20% 超）。

#### Tail/ルーティング設定メモ
- Tempo/Jaeger Collector で `probabilistic + tail` の二段構成を使用し、`error=true` または `orca.result!=00` を含むスパンは常時保持。批判度の高いトランザクション（会計/監査/エクスポート/SSE 初期化）は `span.name` プレフィックスでマッチさせ、環境ラベル `deployment.environment` をキーにルーティング。
- 施設別制限は `baggage.facility_id` をハッシュした deterministic sampler を前段に置き、同施設の連続ドロップを防止。`sampling.limiter.exceeded` をメトリクス化し Alert A5 と共有する。
- バックプレッシャ発生時は Collector 側で一時的に 1% へ縮退し、`runbook: logs/20251122T071146Z-tracing.md#tail-sampling-switch` を参照して復旧順序（Exporter → Tail Processor → Batch サイズ）を適用。

### Web クライアント TraceContext 伝搬要件
- **ブラウザ計測**: OpenTelemetry Web SDK（XHR/Fetch auto-instrumentation）でユーザー操作ごとに Root Span を生成し、`traceparent`/`tracestate`/`baggage` を送信。`baggage` には `facility.id` `user.role` のみをハッシュ化して格納し、個人情報は含めない。
- **API 呼び出し**: `fetch`/`XMLHttpRequest` には `traceparent` を必ず付与し、CORS 設定で `Access-Control-Expose-Headers: traceparent,tracestate` を許可。SSE/WebSocket は初回ハンドシェイクのクエリまたはヘッダで TraceContext を渡し、サーバー側が受け取った値を JMS 送信時にも継承する。
- **リトライ/フォールバック**: UI リトライ時は同一 TraceId を維持し、`baggage.retry_count` を増分。エラー画面に TraceId を表示し、サポート問い合わせ・監査ログと突合できるよう統一する。

#### 追加要件（RUN_ID=`20251122T071146Z`）
- **ヘッダ/属性上限**: `baggage` は 2 キー以内、各 128 文字以内とし、`traceparent`/`tracestate` は仕様上限に従う。サーバーで 4KB を超えた場合は `tracecontext.truncated=true` をログに出し Warning を発火。
- **RUM/メトリクスとの突合**: Web SDK の `resource` 属性に `app.version` `deployment.environment` `feature_flag` を含め、Tempo 側でサーバースパンと `traceId` をキーに結合。LCP/CLS などのフロント指標は `front.*` ラベルを付けてサーバー側と区別する。
- **SSE/WebSocket 継承**: SSE チャネルは再接続時も初回 `traceparent` を再利用し、サーバーが `X-Trace-Id` をレスポンスヘッダに返す。UI は受領した TraceId をデバッグパネルに表示し、JMS/ジョブ関連ログとリンクする。
- **CORS とセキュリティ**: `Access-Control-Allow-Headers` に `traceparent,tracestate,baggage` を追加しつつ、`X-ORCA-Session` などの認証ヘッダとは別に扱う。OPTIONS 応答に Trace ヘッダを漏らさないこと、`baggage` に PII を入れないことを lint（ESLint ルール）で検査する。

### アラートと運用メモ
- Trace サンプリング Drop 率が 5 分平均 10% 超で Warning、20% 超で Critical。ソースは `otel_sampler_dropped_total` を Prometheus 監視。
- JMS メッセージで TraceContext 欠落率が 1% 超の場合、`messaging_tracecontext_missing_total` を警告し、UI 伝搬かサーバー転送かを切り分ける手順を `docs/server-modernization/phase2/operations/logs/20251122T071146Z-tracing.md` に記録する。

## ダッシュボード/アラート設計（RUN_ID=`20251122T071146Z`）
- 証跡: `docs/server-modernization/phase2/operations/logs/20251122T071146Z-alerts.md`（本追補の要約を記録済み）
- 実装サンプルは証跡参照: `docs/server-modernization/phase2/operations/logs/20251122T071146Z-alerts.md#4-a1〜a10-promql--recording-rule-サンプル`

### ダッシュボード案
| 目的/利用者 | 主観点 (KPI) | データソース/UID | 更新・運用 | 備考 |
| --- | --- | --- | --- | --- |
| 運用（Ops） | API p95/99 レイテンシ、5xx/ORCA エラー率、DB/Hikari 接続率、JMS キュー滞留、SSE 中断率 | Prometheus/Loki/Grafana `obs-ops-main`（UID: `obs-ops-main`） | 5m 自動更新。アラートと同じラベルセット（env, service, facility）でフィルタ。 | 夜間用の「抑制中」バナーを表示し、メンテナンス区間は灰色帯で表示。 |
| 開発（Dev） | 新リリース後の回帰指標（エラー率/latency diff）、A/B 実験の SLO 消費、トレース例外トップ | Prometheus + Tempo/Jaeger、Loki クエリ保存 `obs-dev-release` | デプロイから 24h 強調。差分カードで「前ビルド比」を表示。 | Feature flag 名でフィルタ可能にし、rollout 失敗を早期検知。 |
| ビジネス（Biz/CS） | 受付数/請求件数/ORCA 成功率、アクティブユーザー、主要ページ応答時間 (p95) | ClickHouse/BI 抽出 + Prometheus recording rule `biz_kpi_*`、Grafana `obs-biz-kpi` | 15m 更新。月次/週次ビューを切替可能。 | PII を含まない集計のみ。SLO 越えは Slack #biz-alerts へ要約のみ送信。 |

### 主要アラートルール（Alertmanager → Slack/PagerDuty）
| No | シグナル | Warning | Critical | 検知/抑止 | エスカレーション |
| --- | --- | --- | --- | --- | --- |
| A1 API レイテンシ | p95 > 300ms 10m 平均 | p99 > 800ms が 3/5 評価間隔 | `for:10m`、`group_wait=30s`、`group_interval=5m`。メンテ window は `time_interval` で抑止。 | Warning: Slack `#modernized-ops`、Critical: PagerDuty ops-primary → SRE 当番。 |
| A2 API エラー率 | 5xx > 0.5% (5m) | 5xx > 1% 連続 10m または ORCA 正規化コード !=00 が 2% | 直近デプロイの `version` ラベルで分岐し、canary のみで止めるルールを併設。 | Critical は当番 5 分内 ACK、15 分で EM（Mgr）。 |
| A3 DB プール枯渇 | `db_pool_active/max > 0.75` 5m | `>0.90` 5m | 抑止: バッチメンテ window、夜間は Warning のみ。 | Ops 候補: ①接続リーク調査 runbook 起動 ②再起動可否判断。 |
| A4 JMS/ジョブ滞留 | `queue_depth > 500` 10m または `job_queue_wait_p95 > 60s` | `queue_depth > 1000` 10m | Retry spike（`retry.count`>2）を検知したら A5 と連動し 1h サプレッション。 | Slack → PagerDuty secondary（バッチ担当）。 |
| A5 Trace サンプリング Drop | `otel_sampler_dropped_total` rate >10% 10m | >20% 10m | デプロイ直後 15m は抑止。原因特定までは手動サンプリングに切替。 | Slack + Toil backlog に記録、継続時は SRE へ引き継ぎ。 |
| A6 ログ取りこぼし | `log_processing_errors_total` > 50/min 5m | > 200/min 5m | Loki/Elastic 片系だけ死活の場合は Warning に留め、両系停止で Critical。 | On-call が Collector 再起動、復旧不可なら infra チームへ電話。 |
| A7 監査ログ欠損 | `d_audit_event` 挿入失敗率 >0.5% 10m | >1% 10m もしくは 10 分連続 0 件 | 本番のみ有効。夜間も抑止しない。 | Critical は即 PagerDuty + セキュリティ連絡先（メールリスト）。 |
| A8 ORCA 連携 | `orca_bridge_calls_total{result!="00"} >2%` 15m | `>5%` 15m または `system unreachable` 3 分 | Trial/Local ではミュート。本番は維持。 | CS 連絡用の要約を Slack #biz-alerts に同報。 |
| A9 SSE 切断 | `sse_disconnect_total{reason!="client"} / sse_connections_total >0.2%` 1h | >1% 30m | 夜間は Warning のみ。保守 window は抑止。 | Ops 当番が Undertow ログと Tempo で原因突合。 |
| A10 デプロイ後回帰 | 新 version のエラー率が旧比 +0.5pp 超 10m | +1.0pp 超 10m | `version` ラベルで diff。Canary モード中は PagerDuty 抑止。 | Release マネージャーへ DM。必要ならロールバック実施。 |

### 夜間運用・メンテナンスウィンドウ
- 夜間一次対応: 22:00–07:00 JST は Critical のみ PagerDuty（ops-primary）。Warning は Slack に記録し、07:30 の朝会でサマリをレビュー。
- 定期メンテナンス: Stage=毎週水曜 02:00–03:00 JST、Prod=毎月第2日曜 02:00–04:00 JST。Alertmanager `mute_time_intervals` で A1/A2/A3/A4/A9 をサプレッションし、ダッシュボードに灰帯を表示。Stage 実測済み（RUN_ID=`20251122T071146Z`, Alertmanager 0.27 を stage config で起動し、DeployWindow による inhibition で A1/A2 を suppressed、`mute_time_intervals` で A4/A9 通知無しを確認。検証専用の土曜終日 interval は Stage 反映時に削除する）。
- デプロイ抑止: デプロイ後 15 分間は A1/A2/A5/A10 を `for:10m` + `label:deployed_within=15m` で自動抑止。Rollback 時は解除。
- エスカレーション連絡先: Ops当番 → SRE 2nd → サービスオーナー（最大 30 分以内）。Biz 影響（A8, A9）は CS チームへサマリのみ送信。
- 本番適用ステータス: Alertmanager 本番環境への `mute_time_intervals`/`inhibit_rules` 適用は kubeconfig 未提供のため保留中。入手次第 Stage 同等の検証を実施し、本章・証跡・DOC_STATUS を更新する。

## 関連手順書（RUN_ID=`20251122T071146Z`）
- Web クライアント運用: `docs/web-client/operations/otel-webclient-init.md`（ブラウザ OTel 初期化/TraceId 表示）、`docs/web-client/operations/mac-dev-login.local.md`（Dev ログイン手順）、`docs/web-client/operations/LOCAL_BACKEND_DOCKER.md`（ローカルバックエンド起動）、`docs/web-client/operations/TEST_SERVER_DEPLOY.md`（テストサーバー配備フロー）
- Phase2 モダナイズ運用: `docs/server-modernization/phase2/operations/ORCA_CONNECTIVITY_VALIDATION.md`、`docs/server-modernization/phase2/operations/ORCA_API_STATUS.md`、`docs/server-modernization/phase2/operations/MODERNIZED_API_DOCUMENTATION_GUIDE.md`、`docs/server-modernization/phase2/operations/TRACE_PROPAGATION_CHECK.md`、`docs/server-modernization/phase2/operations/TRACEID_JMS_RUNBOOK.md`、`docs/server-modernization/phase2/operations/WILDFLY33_MICROMETER_OPERATIONS_GAP.md`、`docs/server-modernization/phase2/operations/SERVER_MODERNIZED_STARTUP_BLOCKERS.md`
- 証跡ログ: `docs/server-modernization/phase2/operations/logs/20251122T071146Z-links.md`
- UI 証跡: 実バックエンドで 500 応答を発生させた際の TraceErrorBoundary／TraceNoticeCenter（TraceId/RequestId 一致）スクショを `artifacts/observability/20251122T071146Z/traceid-error-boundary.png` に保存。
- 備考: レガシー資産（`client/`, `common/`, `ext_lib/`）は参照専用。
