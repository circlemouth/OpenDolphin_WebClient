# ORCA-05/06/08 Alerting rules (draft)
# 使い方: 環境ごとに envsubst で ${...} を置換して Prometheus/Alertmanager へ適用する。
# RUN_ID=${RUN_ID:-20251124T133000Z}

---
# Prometheus alerting rules (PrometheusRule)
groups:
  - name: orca-master.rules
    interval: 30s
    rules:
      - alert: OrcaMasterHighErrorRate
        expr: |
          sum by(job) (
            rate(opendolphin_api_error_total{path=~"/(orca|api)/orca/(master|tensu).*"}[5m])
          ) /
          sum by(job) (
            rate(opendolphin_api_request_total{path=~"/(orca|api)/orca/(master|tensu).*"}[5m])
          )
          > ${ORCA_ERR_THRESHOLD:-0.02}
        for: 5m
        labels:
          severity: critical
          run_id: "${RUN_ID:-20251124T133000Z}"
          service: orca-master
        annotations:
          summary: "ORCA master 5xx rate >2% (5m)"
          description: |
            5xx rate on ORCA master/tensu endpoints exceeded {{ $value | humanizePercentage }} over 5m.
            path filter: /(orca|api)/orca/(master|tensu).*
            Follow rollback rules in orca-master-release-plan.md §3-5.

      - alert: OrcaMasterP99LatencyHigh
        expr: |
          histogram_quantile(
            0.99,
            sum by (le, job) (
              rate(opendolphin_api_request_duration_seconds_bucket{path=~"/(orca|api)/orca/(master|tensu).*"}[10m])
            )
          ) > ${ORCA_P99_THRESHOLD:-3}
        for: 10m
        labels:
          severity: critical
          run_id: "${RUN_ID:-20251124T133000Z}"
          service: orca-master
        annotations:
          summary: "ORCA master P99 > 3s (10m)"
          description: |
            P99 latency for ORCA master/tensu endpoints exceeded {{ $value | humanizeDuration }} over 10m.
            Check cache/DB and see ORCA_CONNECTIVITY_VALIDATION.md §7.

      - alert: OrcaMasterMissingRatioHigh
        expr: |
          sum by(job) (
            rate(opendolphin_api_request_total{path=~"/(orca|api)/orca/(master|tensu).*",missing_master="true"}[5m])
          ) /
          sum by(job) (
            rate(opendolphin_api_request_total{path=~"/(orca|api)/orca/(master|tensu).*"}[5m])
          ) > ${ORCA_MISSING_THRESHOLD:-0.005}
        for: 5m
        labels:
          severity: warning
          run_id: "${RUN_ID:-20251124T133000Z}"
          service: orca-master
        annotations:
          summary: "missingMaster > 0.5% (5m)"
          description: |
            missingMaster ratio crossed {{ $value | humanizePercentage }} over 5m.
            Verify snapshot availability and cache population.

      - alert: OrcaMasterCacheHitLow
        expr: |
          sum by(job) (
            rate(opendolphin_api_request_total{path=~"/(orca|api)/orca/(master|tensu).*",cache_hit="true"}[15m])
          ) /
          sum by(job) (
            rate(opendolphin_api_request_total{path=~"/(orca|api)/orca/(master|tensu).*"}[15m])
          ) < ${ORCA_CACHE_HIT_MIN:-0.8}
        for: 15m
        labels:
          severity: warning
          run_id: "${RUN_ID:-20251124T133000Z}"
          service: orca-master
        annotations:
          summary: "cacheHit ratio < 80% (15m)"
          description: |
            Cache hit ratio dropped below target for ORCA master/tensu endpoints over 15m.
            Check ETag/TTL settings and Redis availability.

      - alert: OrcaMasterAuditMissing
        expr: |
          sum(rate(opendolphin_api_audit_missing_total{path=~"/(orca|api)/orca/(master|tensu).*"}[5m]))
          /
          sum(rate(opendolphin_api_request_total{path=~"/(orca|api)/orca/(master|tensu).*"}[5m]))
          > ${ORCA_AUDIT_MISSING_THRESHOLD:-0.001}
        for: 5m
        labels:
          severity: warning
          run_id: "${RUN_ID:-20251124T133000Z}"
          service: orca-master
        annotations:
          summary: "audit fields missing >0.1% (5m)"
          description: |
            Audit fields (runId/dataSource/cacheHit/missingMaster/fallbackUsed/fetchedAt) are missing in >0.1% of responses.
            Flip ORCA_MASTER_AUDIT_ENABLED=false only if instructed; see release plan §5.

---
# Alertmanager route/receivers (template)
route:
  receiver: pagerduty-orca-master
  group_by: [alertname, service]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 30m
  routes:
    - matchers:
        - severity="critical"
      receiver: pagerduty-orca-master
    - matchers:
        - severity="warning"
      receiver: email-orca-master

receivers:
  - name: pagerduty-orca-master
    pagerduty_configs:
      - routing_key: "${PAGERDUTY_ORCA_MASTER_KEY:-REPLACE_WITH_PD_KEY}"
        severity: '{{ if eq .Labels.severity "critical" }}critical{{ else }}warning{{ end }}'
        class: "orca-master"
        component: "backend"
        group: '{{ .Labels.service }}'
        details:
          run_id: "${RUN_ID:-20251124T133000Z}"
          alertname: '{{ .Labels.alertname }}'
          path: '{{ .Labels.path }}'
  - name: email-orca-master
    email_configs:
      - to: "${ORCA_ALERT_EMAILS:-ops@example.com,dev@example.com}"
        send_resolved: true
        headers:
          Subject: "[ORCA Master][${RUN_ID:-20251124T133000Z}] {{ .CommonLabels.alertname }}"
